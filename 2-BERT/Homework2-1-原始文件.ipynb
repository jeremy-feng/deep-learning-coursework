{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业2：对Bert进行微调，完成QA任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如果你对Bert没有了解，请先观看视频 [BERT 论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1PL411M7eQ)**\n",
    "\n",
    "注：本次作业并不需要预先了解任何Transformer的知识，如有兴趣，可以在观看Bert的视频前，先预习 [Transformer论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1pu411o7BE)，后续课程中会讲解Transformer的知识。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jeremy/Desktop/File/一年级下/深度学习/deep-learning-coursework/2 - RNN\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/cmrc2018/train.json') as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "with open('../input/cmrc2018/dev.json') as f:\n",
    "    dev = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a glance at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraphs': [{'id': 'TRAIN_186',\n",
       "   'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       "   'qas': [{'question': '范廷颂是什么时候被任为主教的？',\n",
       "     'id': 'TRAIN_186_QUERY_0',\n",
       "     'answers': [{'text': '1963年', 'answer_start': 30}]},\n",
       "    {'question': '1990年，范廷颂担任什么职务？',\n",
       "     'id': 'TRAIN_186_QUERY_1',\n",
       "     'answers': [{'text': '1990年被擢升为天主教河内总教区宗座署理', 'answer_start': 41}]},\n",
       "    {'question': '范廷颂是于何时何地出生的？',\n",
       "     'id': 'TRAIN_186_QUERY_2',\n",
       "     'answers': [{'text': '范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生',\n",
       "       'answer_start': 97}]},\n",
       "    {'question': '1994年3月，范廷颂担任什么职务？',\n",
       "     'id': 'TRAIN_186_QUERY_3',\n",
       "     'answers': [{'text': '1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理',\n",
       "       'answer_start': 548}]},\n",
       "    {'question': '范廷颂是何时去世的？',\n",
       "     'id': 'TRAIN_186_QUERY_4',\n",
       "     'answers': [{'text': '范廷颂于2009年2月22日清晨在河内离世', 'answer_start': 759}]}]}],\n",
       " 'id': 'TRAIN_186',\n",
       " 'title': '范廷颂'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "\n",
    "# You can explore more pretrained models from https://huggingface.co/models\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-chinese').to(device)\n",
    "\n",
    "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "questions = []\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "for paragraph in train['data']:\n",
    "    for qa in paragraph['paragraphs'][0]['qas']:\n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # For each question, add its paragraph, question, start_position and end_position(after calculation) to its corresponding list.\n",
    "        paragraphs.append(###)\n",
    "        questions.append(###)\n",
    "        start_positions.append(###)\n",
    "        end_positions.append(###)\n",
    "        ### END CODE HERE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(paragraphs, questions, \n",
    "                            return_tensors='pt', padding=True, truncation=True,\n",
    "                           max_length=512)\n",
    "\n",
    "# `char_to_token` will convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
    "train_encodings['start_positions'] = torch.tensor([train_encodings.char_to_token(idx, x) if train_encodings.char_to_token(idx, x) != None else -1\n",
    "                                      for idx, x in enumerate(start_positions)])\n",
    "train_encodings['end_positions'] = torch.tensor([train_encodings.char_to_token(idx, x-1) if train_encodings.char_to_token(idx, x-1) != None else -1\n",
    "                                    for idx, x in enumerate(end_positions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在问答任务中，`input_ids` 是将输入文本转换为整数序列后的输出。它将每个单词或子词映射到一个唯一的整数 ID, 位于 [CLS] 和 [SEP] 标记会被分别映射到一个特殊的 ID，(101: CLS, 102: SEP)。具体可以参考下方例子。\n",
    "\n",
    "- 在 `token_type_ids` 中，这些标记的值通常为 0 或 1，其中 0 表示该 token 属于第一个文本序列（通常是问题），1 表示该 token 属于第二个文本序列（通常是段落）。\n",
    "\n",
    "- 在 `attention_mask` 中，0 表示对应的标记应该被忽略，1 表示对应的标记应该被关注。当输入序列长度不足最大长度时，我们需要在序列末尾填充一些无意义的标记，以使序列长度达到最大长度。在这种情况下，`tokenizer`将填充的标记的 attention mask 设置为 0，以告诉模型它们不应该被关注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 5745, 2455,  ..., 4638, 8043,  102],\n",
       "        [ 101, 5745, 2455,  ..., 1218, 8043,  102],\n",
       "        [ 101, 5745, 2455,  ..., 4638, 8043,  102],\n",
       "        ...,\n",
       "        [ 101, 7027, 1305,  ...,    0,    0,    0],\n",
       "        [ 101, 7027, 1305,  ...,    0,    0,    0],\n",
       "        [ 101,  517, 4263,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10142, 512])\n",
      "torch.Size([10142, 512])\n",
      "torch.Size([10142, 512])\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['input_ids'].shape)\n",
    "print(train_encodings['token_type_ids'].shape)\n",
    "print(train_encodings['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "    \n",
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx].to(device) for k, v in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Mixed Precision (AMP) is available on NVIDIA GPUs that support Tensor Cores, which are specialized hardware units for performing fast matrix multiplication and convolution operations in deep learning. Specifically, Tensor Cores are available on NVIDIA Volta, Turing, and Ampere architectures, which include the following GPU series:\n",
    "\n",
    "- Volta: Tesla V100, Titan V\n",
    "- Turing: Quadro RTX, GeForce RTX 20-series, Titan RTX\n",
    "- Ampere: A100, GeForce RTX 30-series, Titan RTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.23.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.12.1+cu116)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.9.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->accelerate) (4.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\n",
    "fp16_training = True\n",
    "\n",
    "if fp16_training:\n",
    "    !pip install accelerate\n",
    "    from accelerate import Accelerator\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1268/1268 [04:11<00:00,  5.04it/s, Epoch 0, train loss: 2.0872, acc start: 0.4315, acc end: 0.4256]\n",
      "Epoch 1: 100%|██████████| 1268/1268 [04:12<00:00,  5.03it/s, Epoch 1, train loss: 1.1910, acc start: 0.5964, acc end: 0.6007]\n",
      "Epoch 2: 100%|██████████| 1268/1268 [04:11<00:00,  5.04it/s, Epoch 2, train loss: 0.8371, acc start: 0.6862, acc end: 0.6900]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "### START CODE HERE ### \n",
    "# Use AdamW as the optimizer, and learning rate 5e-5.\n",
    "# https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "optim = \n",
    "### END CODE HERE ### \n",
    "\n",
    "\n",
    "if fp16_training:\n",
    "    model, optim, train_loader = accelerator.prepare(model, optim, train_loader)\n",
    "    \n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    loss_sum = 0.0\n",
    "    acc_start_sum = 0.0\n",
    "    acc_end_sum = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        start_positions = batch['start_positions']\n",
    "        end_positions = batch['end_positions']\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, \n",
    "                        start_positions=start_positions, \n",
    "                        end_positions=end_positions)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        if fp16_training:\n",
    "            accelerator.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # Obtain answer by choosing the most probable start position / end position\n",
    "        # Using `torch.argmax` and its `dim` parameter to extract preditions for start position and end position.\n",
    "        start_pred = \n",
    "        end_pred = \n",
    "        \n",
    "        # calculate accuracy for start and end positions. eg., using start_pred and start_positions to calculate acc_start.\n",
    "        acc_start =\n",
    "        acc_end = \n",
    "        ### END CODE HERE ### \n",
    "        \n",
    "        acc_start_sum += acc_start\n",
    "        acc_end_sum += acc_end\n",
    "        \n",
    "        # Update progress bar\n",
    "        postfix = {\n",
    "            \"loss\": f\"{loss_sum/(batch_idx+1):.4f}\",\n",
    "            \"acc_start\": f\"{acc_start_sum/(batch_idx+1):.4f}\",\n",
    "            \"acc_end\": f\"{acc_end_sum/(batch_idx+1):.4f}\"\n",
    "        }\n",
    "\n",
    "        # Add batch accuracy to progress bar\n",
    "        batch_desc = f\"Epoch {epoch}, train loss: {postfix['loss']}\"\n",
    "        pbar.set_postfix_str(f\"{batch_desc}, acc start: {postfix['acc_start']}, acc end: {postfix['acc_end']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predcit(doc, query):\n",
    "    print(doc)\n",
    "    print('提问：', query)\n",
    "    item = tokenizer([doc, query], max_length=512, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        input_ids = item['input_ids'].to(device).reshape(1,-1)\n",
    "        attention_mask = item['attention_mask'].to(device).reshape(1,-1)\n",
    "        \n",
    "        outputs = model(input_ids[:, :512], attention_mask[:, :512])\n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # Using `torch.argmax` and its `dim` parameter to extract preditions for start position and end position.\n",
    "        start_pred = \n",
    "        end_pred = \n",
    "        ### END CODE HERE ### \n",
    "    \n",
    "    try:\n",
    "        start_pred = item.token_to_chars(0, start_pred)\n",
    "        end_pred = item.token_to_chars(0, end_pred)\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    if start_pred.start > end_pred.end:\n",
    "        return ''\n",
    "    else:\n",
    "        return doc[start_pred.start:end_pred.end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraphs': [{'id': 'DEV_109',\n",
       "   'context': '岑朗天（），笔名朗天、霍惊觉。香港作家、影评人、文化活动策划、大学兼职讲师。香港新亚研究所硕士，师从牟宗三，父亲为香港专栏作家昆南。曾在香港多家报社从事繙译、编辑、采访工作。1995年加入香港电影评论学会，并于2003-2007年出任该会会长，2016年退出。1995年参与创立新研哲学会，后易名香港人文哲学会，再易名香港人文学会。1998年加入树宁．现在式单位，出任该剧团董事及编剧。2003年担任牛棚书展（2003-6）统筹，协助开拓主流以外的书展文化（牛棚书展精神后为九龙城书节继承）。2004年6月至2011年加入商业电台光明顶，担任嘉宾主持。2004年至2014年于香港中文大学新闻与传播学院兼职教授媒体创意写作。2012年始兼任香港浸会大学电影学院讲师，教授文学与影视相关课程。',\n",
       "   'qas': [{'question': '岑朗天笔名叫什么？',\n",
       "     'id': 'DEV_109_QUERY_0',\n",
       "     'answers': [{'text': '朗天、霍惊觉', 'answer_start': 8},\n",
       "      {'text': '朗天、霍惊觉', 'answer_start': 8},\n",
       "      {'text': '朗天、霍惊觉', 'answer_start': 8}]},\n",
       "    {'question': '岑朗天的职业都有哪些？',\n",
       "     'id': 'DEV_109_QUERY_1',\n",
       "     'answers': [{'text': '作家、影评人、文化活动策划、大学兼职讲师', 'answer_start': 17},\n",
       "      {'text': '作家、影评人、文化活动策划、大学兼职讲师', 'answer_start': 17},\n",
       "      {'text': '作家、影评人、文化活动策划、大学兼职讲师', 'answer_start': 17}]},\n",
       "    {'question': '岑朗天哪年加入香港电影评论学会？',\n",
       "     'id': 'DEV_109_QUERY_2',\n",
       "     'answers': [{'text': '1995年', 'answer_start': 87},\n",
       "      {'text': '1995年', 'answer_start': 87},\n",
       "      {'text': '1995年', 'answer_start': 87}]},\n",
       "    {'question': '2004年6月至2011年，岑朗天在什么地方担任嘉宾主持？',\n",
       "     'id': 'DEV_109_QUERY_3',\n",
       "     'answers': [{'text': '商业电台光明顶', 'answer_start': 261},\n",
       "      {'text': '商业电台光明顶', 'answer_start': 261},\n",
       "      {'text': '商业电台光明顶', 'answer_start': 261}]},\n",
       "    {'question': '2004年至2014年，岑朗天在香港中文大学哪个学院教授课程？',\n",
       "     'id': 'DEV_109_QUERY_4',\n",
       "     'answers': [{'text': '新闻与传播学院', 'answer_start': 294},\n",
       "      {'text': '新闻与传播学院', 'answer_start': 294},\n",
       "      {'text': '新闻与传播学院', 'answer_start': 294}]}]}],\n",
       " 'id': 'DEV_109',\n",
       " 'title': '岑朗天'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev['data'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "岑朗天（），笔名朗天、霍惊觉。香港作家、影评人、文化活动策划、大学兼职讲师。香港新亚研究所硕士，师从牟宗三，父亲为香港专栏作家昆南。曾在香港多家报社从事繙译、编辑、采访工作。1995年加入香港电影评论学会，并于2003-2007年出任该会会长，2016年退出。1995年参与创立新研哲学会，后易名香港人文哲学会，再易名香港人文学会。1998年加入树宁．现在式单位，出任该剧团董事及编剧。2003年担任牛棚书展（2003-6）统筹，协助开拓主流以外的书展文化（牛棚书展精神后为九龙城书节继承）。2004年6月至2011年加入商业电台光明顶，担任嘉宾主持。2004年至2014年于香港中文大学新闻与传播学院兼职教授媒体创意写作。2012年始兼任香港浸会大学电影学院讲师，教授文学与影视相关课程。\n",
      "提问： 岑朗天笔名叫什么？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'朗天、霍惊觉'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "predcit(dev['data'][100]['paragraphs'][0]['context'],\n",
    "       dev['data'][100]['paragraphs'][0]['qas'][0]['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Questions\n",
    "可以查阅相关资料，并完成如下开放式的问答题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们使用了512长度的Bert，但是在实际应用中，输入长度可能大于512，你想怎么解决这个问题，请描述你的算法，在训练和预测时分别采取什么样的方法。（假设问题的长度都满足小于512token，段落的长度可能大于512token，以QA问题为例）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在输出中，我们分别对start_pred和end_pred的位置进行预估，如果end_pred<start_pred，我们可以如何解决这样的问题?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bert的分词方式是什么?在中文中，你觉得这样的方式会带来什么问题？什么样的分词方式适合中文？在中文的文本上，除了改变分词方式，还有哪些方式可以提升模型效果？\n",
    "\n",
    "阅读资料：https://github.com/ymcui/Chinese-BERT-wwm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
