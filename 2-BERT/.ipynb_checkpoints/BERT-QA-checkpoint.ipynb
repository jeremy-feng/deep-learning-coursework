{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeremy-feng/deep-learning-coursework/blob/main/2-BERT/BERT-QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XF4wAqb4yOr"
   },
   "source": [
    "# 作业2：对Bert进行微调，完成QA任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeoLEi2r4yOt"
   },
   "source": [
    "**如果你对Bert没有了解，请先观看视频 [BERT 论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1PL411M7eQ)**\n",
    "\n",
    "注：本次作业并不需要预先了解任何Transformer的知识，如有兴趣，可以在观看Bert的视频前，先预习 [Transformer论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1pu411o7BE)，后续课程中会讲解Transformer的知识。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUrLd3HX71mx",
    "outputId": "e2420273-e24c-40d4-9ceb-fef9785999bd"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WG84QaHe4yOu",
    "outputId": "9d4d1073-7a14-4c68-c8a0-bd5c1aec0443",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gemini/code/2-BERT\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "261wXIir4yOv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xDKJnLW24yOv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./input/cmrc2018/train.json') as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "with open('./input/cmrc2018/dev.json') as f:\n",
    "    dev = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKFgHzeK4yOv"
   },
   "source": [
    "Let's have a glance at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8-oDM8-4yOv",
    "outputId": "db5fc040-d3f0-49dc-9c88-3f671bb7a3cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.28.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /gemini/bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at /gemini/bert-base-chinese and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "\n",
    "# You can explore more pretrained models from https://huggingface.co/models\n",
    "# 这段代码利用Hugging Face库中的BertTokenizerFast方法从预训练模型'bert-base-chinese'中加载tokenizer。\n",
    "# 这个预训练模型是一个中文BERT模型，可以将中文句子或文本数据转换为相应的token，以便进行文本分类、序列标注等自然语言处理任务。\n",
    "# BertTokenizerFast是BertTokenizer的升级版，速度更快，性能更优。\n",
    "tokenizer = BertTokenizerFast.from_pretrained('/gemini/bert-base-chinese')\n",
    "model = BertForQuestionAnswering.from_pretrained('/gemini/bert-base-chinese').to(device)\n",
    "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用多块 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "# 将模型包装在DataParallel中\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zm-mWAfp4yOv"
   },
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWLFri4V4yOv"
   },
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "W4Vq3rCC4yOv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "questions = []\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "for paragraph in train['data']:\n",
    "    for qa in paragraph['paragraphs'][0]['qas']:\n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # For each question, add its paragraph, question, start_position and end_position(after calculation) to its corresponding list.\n",
    "        paragraphs.append(paragraph['paragraphs'][0]['context'])\n",
    "        questions.append(qa['question'])\n",
    "        start_position = qa['answers'][0]['answer_start']\n",
    "        start_positions.append(start_position)\n",
    "        anwser_length = len(qa['answers'][0]['text'])\n",
    "        end_positions.append(start_position + anwser_length)\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYpWVxhvE_5l",
    "outputId": "55fa7c9d-1594-440d-c7e4-56581cb365d5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['范廷颂是什么时候被任为主教的？',\n",
       " '1990年，范廷颂担任什么职务？',\n",
       " '范廷颂是于何时何地出生的？',\n",
       " '1994年3月，范廷颂担任什么职务？',\n",
       " '范廷颂是何时去世的？']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxwRkayQFGXR",
    "outputId": "91fe224b-7871-4e6c-ef47-555cf471535a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 41, 97, 548, 759]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_positions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pshpsS3BFJFh",
    "outputId": "3e1b98f9-bba9-4ceb-844b-2926d150828a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 62, 126, 598, 780]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_positions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbQdrGUFilrE"
   },
   "source": [
    "查看第 3 个问题的回答是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "hXwZrYtQiqWY",
    "outputId": "623bcfb6-869c-43df-f815-3d941b741cb2",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[0][start_positions[2]: end_positions[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAf1IrcPGGch"
   },
   "source": [
    "将 paragraphs 和 questions 进行 encoding。\n",
    "参考：[https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BtlaETbX4yOw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 下面这段代码使用了 Hugging Face 的 tokenizer 方法，将 paragraphs 和 questions 转换成相应的 token，\n",
    "# 返回一个字典 (train_encodings) 包含这些 token 的各种信息，这些信息包括 input_ids、attention_mask 等等。\n",
    "# return_tensors='pt' 表示返回 PyTorch 下的 tensor 格式\n",
    "# padding 用于填充不足 max_length 的 token\n",
    "# truncation 用于在超过 max_length 时截断 token\n",
    "# 最终的 token 长度被限制在 512 内\n",
    "train_encodings = tokenizer(\n",
    "    paragraphs,\n",
    "    questions,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQAzqnsvIFqD",
    "outputId": "1fa893cf-ed13-478f-b2f9-5e2ffe9b0475",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX4vyeGS4yOw"
   },
   "source": [
    "- 在问答任务中，`input_ids` 是将输入文本转换为整数序列后的输出。它将每个单词或子词映射到一个唯一的整数 ID, 位于 [CLS] 和 [SEP] 标记会被分别映射到一个特殊的 ID，(101: CLS, 102: SEP)。具体可以参考下方例子。\n",
    "\n",
    "- 在 `token_type_ids` 中，这些标记的值通常为 0 或 1，其中 0 表示该 token 属于第一个文本序列（通常是问题），1 表示该 token 属于第二个文本序列（通常是段落）。\n",
    "\n",
    "- 在 `attention_mask` 中，0 表示对应的标记应该被忽略，1 表示对应的标记应该被关注。当输入序列长度不足最大长度时，我们需要在序列末尾填充一些无意义的标记，以使序列长度达到最大长度。在这种情况下，`tokenizer`将填充的标记的 attention mask 设置为 0，以告诉模型它们不应该被关注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZeEfNKVyWAen",
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = tokenizer(\n",
    "    paragraphs[0],\n",
    "    questions[0],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CQbP9QEJZ-xS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = tokenizer(\n",
    "    paragraphs[0],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-TCD9U374yOw",
    "outputId": "d6b6733b-df66-4150-e6c1-7648e6f013ad",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'为 天 主 教 河 内 总 教 区 宗 座 署 [SEP] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([711, 1921,  712, 3136, 3777, 1079, 2600, 3136, 1277,\n",
    "        2134, 2429, 5392,  102, 5745, 2455, 7563, 3221,  784,  720, 3198,  952,\n",
    "        6158,  818,  711,  712, 3136, 4638, 8043,  102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l80_9ZsedeMq",
    "outputId": "2570c87a-4ba7-428b-a10b-d290645de170",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([5745, 2455, 7563, 3221,  784,  720, 3198,  952,\n",
    "        6158,  818,  711,  712, 3136, 4638, 8043,  102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2knZqNTO6ePb"
   },
   "source": [
    "最后 16 个元素为 1，这说明最后 16 个 token 对应的是 question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJ9lOj7vdqkS",
    "outputId": "7dc3b5c1-e3ec-4dd2-fc34-bfcecf168cca",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['token_type_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHmjrpNqdgV3",
    "outputId": "b3483015-f0f5-40f1-977c-5fa58ab8d191",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['token_type_ids'][0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "seEeGEBLdDsp",
    "outputId": "ce72ea78-62e5-43d4-c73d-d7e961ed543e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 [SEP]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([711, 1921,  712, 3136, 3777, 1079, 2600, 3136, 1277,\n",
    "        2134, 2429, 5392, 4415,  809, 1856, 6133, 6421, 3136, 1277, 2600,  712,\n",
    "        3136, 4638, 4958, 5375,  511, 8447, 2399,  102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "izDDbdndLW9m",
    "outputId": "02ee7315-cdee-4b11-8694-b48f44e1af18",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dimamZ1Q4yOw",
    "outputId": "78491690-7b59-47a3-9c13-6b83f39847b2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10142, 512])\n",
      "torch.Size([10142, 512])\n",
      "torch.Size([10142, 512])\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['input_ids'].shape)\n",
    "print(train_encodings['token_type_ids'].shape)\n",
    "print(train_encodings['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nok-nu4jg6Av"
   },
   "source": [
    "下面的代码的作用是：将 answer 在原始 paragrapgh 的起止索引，转换为在经过tokenizor 之后点 input_ids 中的起止索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xPfmrljfKmal",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `char_to_token` will convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
    "train_encodings['start_positions'] = torch.tensor([train_encodings.char_to_token(idx, x) if train_encodings.char_to_token(idx, x) != None else -1\n",
    "                                      for idx, x in enumerate(start_positions)])\n",
    "train_encodings['end_positions'] = torch.tensor([train_encodings.char_to_token(idx, x-1) if train_encodings.char_to_token(idx, x-1) != None else -1\n",
    "                                    for idx, x in enumerate(end_positions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ok3cM76hjDc",
    "outputId": "e2aae299-28ef-4f81-8462-a0e446af859d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 31,  39,  86,  ..., 142, 225,  17])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['start_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCjrEnwghkpZ",
    "outputId": "c27374e8-c216-40bd-d7b2-06dd55b66f03",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 32,  56, 110,  ..., 143, 244,  19])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['end_positions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "covx-s_zh_Fk"
   },
   "source": [
    "以第 3 个问题为例，即以 86 和 110 作为起止索引为例，查看它们对应的字符串是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2MVfIHvSiRgF",
    "outputId": "ad66e184-6550-4ed2-c33f-17f0e211aa5c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'范廷颂是于何时何地出生的？'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LcgO8Dk6hynX",
    "outputId": "597a27a9-1641-45f1-9cf4-1ed6c1e7bdef",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[0][start_positions[2]: end_positions[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "J5mqFCD3horn",
    "outputId": "d1082623-4bdf-4313-e7de-62df3b0270a6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_encodings['input_ids'][0][86: 110+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBxjey-94yOw"
   },
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "n6j6_ndI4yOw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "    \n",
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx].to(device) for k, v in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrtMKHnx4yOw"
   },
   "source": [
    "Automatic Mixed Precision (AMP) is available on NVIDIA GPUs that support Tensor Cores, which are specialized hardware units for performing fast matrix multiplication and convolution operations in deep learning. Specifically, Tensor Cores are available on NVIDIA Volta, Turing, and Ampere architectures, which include the following GPU series:\n",
    "\n",
    "- Volta: Tesla V100, Titan V\n",
    "- Turing: Quadro RTX, GeForce RTX 20-series, Titan RTX\n",
    "- Ampere: A100, GeForce RTX 30-series, Titan RTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "renVJ9D-4yOx",
    "outputId": "4b99918d-b02b-4a82-93cb-16cf3cedd169",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (0.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.12.1+cu113)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->accelerate) (4.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\n",
    "fp16_training = True\n",
    "\n",
    "if fp16_training:\n",
    "    !pip install accelerate\n",
    "    from accelerate import Accelerator\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFviEE_F8oii",
    "outputId": "07889db5-1aaf-4f2c-c1a2-455f1d966114",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vaFhtF97-glu",
    "outputId": "29e80e84-e934-40d2-d145-9f8894b0db1e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0262,  0.0109, -0.0187,  ...,  0.0903,  0.0028,  0.0064],\n",
       "        [ 0.0021,  0.0216,  0.0011,  ...,  0.0809,  0.0018,  0.0249],\n",
       "        [ 0.0147,  0.0005,  0.0028,  ...,  0.0836,  0.0121,  0.0282],\n",
       "        ...,\n",
       "        [ 0.0346,  0.0021,  0.0085,  ...,  0.0085,  0.0337,  0.0099],\n",
       "        [ 0.0541,  0.0289,  0.0263,  ...,  0.0526,  0.0651,  0.0353],\n",
       "        [ 0.0200,  0.0023, -0.0089,  ...,  0.0799, -0.0562,  0.0247]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdMC5I8d-Jyt",
    "outputId": "9cb111c5-7ee5-4b23-d9e4-5824ed2bd5d8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21128, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "rmerLUao4yOx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "### START CODE HERE ### \n",
    "# Use AdamW as the optimizer, and learning rate 5e-5.\n",
    "# https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "### END CODE HERE ### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "y5zUdeMdAjOz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, optim, train_loader = accelerator.prepare(model, optim, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ehKlMKzzAjtn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_sum = 0.0\n",
    "acc_start_sum = 0.0\n",
    "acc_end_sum = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hnjdfXlsAnyy",
    "outputId": "b32dbf64-6b10-4b0b-ce3e-c9634699de38",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'input_ids': tensor([[ 101, 1102, 3777,  ...,    0,    0,    0],\n",
      "        [ 101, 6437, 1923,  ...,    0,    0,    0],\n",
      "        [ 101, 4549, 4565,  ..., 4638, 8043,  102],\n",
      "        ...,\n",
      "        [ 101, 6205, 2335,  ...,    0,    0,    0],\n",
      "        [ 101,  517, 4263,  ..., 4638, 8043,  102],\n",
      "        [ 101, 7942, 7583,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'start_positions': tensor([ 38, 108,  19,  81,  10, 330, 204,  12, 170,  44, 246, 473, 263,   8,\n",
      "         37, 270], device='cuda:0'), 'end_positions': tensor([ 47, 131,  20,  86,  17, 346, 240,  22, 175,  55, 254, 478, 292,  28,\n",
      "         51, 293], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "    print(batch_idx, batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-guALDwAr03",
    "outputId": "4514bf48-defa-4e87-e793-8301927c23c5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSup9yI_BtYa"
   },
   "source": [
    "由于 train_loader 设置了 batch_size=8，因此这里每一个批次包含 8 个数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGq5IQyfBFo3",
    "outputId": "4b263c75-399e-4f20-a394-582c38bf0feb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1102, 3777,  ...,    0,    0,    0],\n",
       "         [ 101, 6437, 1923,  ...,    0,    0,    0],\n",
       "         [ 101, 4549, 4565,  ..., 4638, 8043,  102],\n",
       "         ...,\n",
       "         [ 101, 6205, 2335,  ...,    0,    0,    0],\n",
       "         [ 101,  517, 4263,  ..., 4638, 8043,  102],\n",
       "         [ 101, 7942, 7583,  ...,    0,    0,    0]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " 'start_positions': tensor([ 38, 108,  19,  81,  10, 330, 204,  12, 170,  44, 246, 473, 263,   8,\n",
       "          37, 270], device='cuda:0'),\n",
       " 'end_positions': tensor([ 47, 131,  20,  86,  17, 346, 240,  22, 175,  55, 254, 478, 292,  28,\n",
       "          51, 293], device='cuda:0')}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "KNMkO5oYCU30",
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "GuPEKq_DCXs2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "input_ids = batch['input_ids']\n",
    "attention_mask = batch['attention_mask']\n",
    "start_positions = batch['start_positions']\n",
    "end_positions = batch['end_positions']\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask, \n",
    "                start_positions=start_positions, \n",
    "                end_positions=end_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eH2jT4s7Ccy9",
    "outputId": "37109c11-231e-49ab-8469-3ed74c100d66",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=tensor([6.2771, 6.3710], device='cuda:0', grad_fn=<GatherBackward>), start_logits=tensor([[ 0.2333, -0.1105,  0.6822,  ...,  0.0231, -0.0222,  0.0102],\n",
       "        [-0.3289, -0.3280,  0.4674,  ..., -0.1779, -0.0131, -0.1010],\n",
       "        [-0.0598, -0.0948,  0.3629,  ...,  0.4607,  0.5397,  0.2173],\n",
       "        ...,\n",
       "        [ 0.4115,  0.0105,  0.3950,  ...,  0.2706,  0.0590, -0.0613],\n",
       "        [ 0.3757,  0.3558,  0.6048,  ...,  1.2805,  0.7974,  0.9003],\n",
       "        [ 0.6398,  0.0018, -0.0332,  ...,  0.0141,  0.0399, -0.0632]],\n",
       "       device='cuda:0', grad_fn=<GatherBackward>), end_logits=tensor([[ 0.3298,  0.2781,  0.0805,  ..., -0.0170, -0.1208,  0.0160],\n",
       "        [ 0.6193,  0.7423, -0.5751,  ...,  0.0174,  0.1072,  0.1716],\n",
       "        [ 0.5311,  0.1346, -0.5384,  ..., -0.7716,  0.4857,  0.2061],\n",
       "        ...,\n",
       "        [ 0.2173,  0.6687,  0.0799,  ..., -0.1546, -0.1969,  0.0433],\n",
       "        [ 0.1835,  0.0966, -0.5411,  ..., -0.3192,  0.5321,  0.2047],\n",
       "        [ 0.6285,  0.0430,  0.1562,  ..., -0.2324, -0.1589,  0.1179]],\n",
       "       device='cuda:0', grad_fn=<GatherBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "UY1BDazICsjQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = outputs.loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVz5XXcMCt0n",
    "outputId": "91839d86-9bd2-4bf3-b07e-a623d7f413d4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.3240, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "K8zfgU2fCyn6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "accelerator.backward(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "DFWK6kxKC008",
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "fqULWs1MC48Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_sum += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsGgLXaJFAE8"
   },
   "source": [
    "对于第一个问答，输出每个 token 作为 start 的 logit，其值越大，说明越有可能作为 start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLtJO3k6Eic_",
    "outputId": "32d7a3d7-5de0-4184-aed7-11e3de433b14",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3331e-01, -1.1049e-01,  6.8216e-01,  4.4067e-01,  6.5761e-01,\n",
       "         7.8543e-01,  8.6340e-01,  3.7774e-01,  3.7006e-01,  1.5304e-01,\n",
       "        -3.4069e-02,  1.9332e-01,  2.7884e-01,  5.8990e-01, -1.0799e-01,\n",
       "         6.1332e-01,  4.5497e-01,  4.1115e-01, -2.3426e-03,  3.2130e-01,\n",
       "         5.3063e-01,  5.9113e-01,  5.6947e-01,  4.5934e-01, -9.7350e-02,\n",
       "         2.6866e-02,  7.1834e-01, -6.1574e-02,  7.3875e-01, -4.8399e-01,\n",
       "        -5.6428e-01,  3.3742e-01, -1.2861e-01,  5.7800e-01,  4.1190e-01,\n",
       "         5.8449e-01, -3.7402e-03, -2.3218e-02,  7.2961e-02,  4.7424e-02,\n",
       "        -1.1624e-01,  6.5343e-01,  1.2771e-02, -1.6841e-01, -3.3608e-01,\n",
       "        -3.1160e-01,  1.2994e-01, -2.2614e-01, -6.8106e-03,  1.0701e-01,\n",
       "         4.1983e-01,  2.0080e-01,  1.7025e-01,  4.5413e-01,  4.6022e-01,\n",
       "         1.3973e-01, -4.9861e-01, -2.4032e-01,  1.4572e-01,  4.3159e-01,\n",
       "         3.1679e-02,  1.5496e-03, -5.0969e-01, -9.2688e-02,  2.2267e-02,\n",
       "        -1.3271e-01, -2.5667e-01,  6.0184e-01, -2.2074e-01,  2.0728e-01,\n",
       "         1.3417e-01, -5.9045e-01,  6.6545e-01,  2.2385e-01,  3.4580e-01,\n",
       "         3.7740e-01,  1.1260e-01,  4.3637e-01,  5.7743e-01,  6.8072e-01,\n",
       "         4.8249e-01, -6.4281e-01, -1.7384e-01, -1.0829e-02, -2.0179e-01,\n",
       "         1.7304e-01,  3.5813e-01,  1.3853e-01,  6.4841e-01,  2.0106e-01,\n",
       "         5.2273e-02, -2.7326e-01, -5.8059e-02,  6.6616e-01,  1.0390e-01,\n",
       "         1.1891e-02,  5.4392e-01,  3.1319e-01, -5.3353e-02, -3.3501e-02,\n",
       "        -5.1094e-01, -1.9478e-01, -3.8766e-01, -3.0078e-01, -3.0362e-01,\n",
       "         3.1369e-02,  6.2005e-01, -6.7144e-01, -1.6525e-01,  2.4225e-01,\n",
       "         3.6885e-01,  9.9606e-01,  1.7816e-01, -4.4254e-01, -6.8813e-01,\n",
       "         3.8279e-01,  3.9012e-01, -6.7267e-01, -7.3370e-01, -1.1869e-01,\n",
       "         5.5970e-01, -2.1427e-02,  2.6586e-03,  6.5396e-01,  1.6374e-01,\n",
       "         5.9081e-01, -4.2210e-04,  1.9063e-01,  1.8975e-01,  5.3235e-01,\n",
       "        -7.3823e-02, -2.7649e-01,  1.7552e-01,  2.7380e-02,  6.9518e-01,\n",
       "         1.3124e-02, -3.3362e-02,  2.4506e-01,  9.4518e-02, -5.8990e-01,\n",
       "        -6.1207e-01, -1.3079e-01,  6.2314e-01,  2.4235e-01, -8.6622e-02,\n",
       "         1.9967e-01, -4.5680e-01, -6.5779e-01, -4.9557e-01, -5.7632e-01,\n",
       "        -1.2922e-01,  5.0816e-02,  1.1054e-01, -4.7603e-01,  1.4637e-01,\n",
       "        -2.4297e-01, -2.5998e-02, -6.8314e-01, -4.0154e-01, -1.4203e-01,\n",
       "         3.0118e-02, -3.9460e-01,  2.6104e-01,  5.1743e-01, -1.9069e-01,\n",
       "         1.6729e-02, -5.7415e-02, -4.4437e-01, -2.7668e-01, -3.8717e-03,\n",
       "         1.7901e-02,  2.4733e-01,  3.9866e-02,  1.8425e-01,  4.3067e-01,\n",
       "         2.8527e-01,  3.1383e-01, -4.0620e-01,  2.7322e-01,  1.5640e-01,\n",
       "         5.3349e-01,  1.9488e-01,  4.8619e-01,  2.6962e-01, -4.9121e-01,\n",
       "         3.5814e-01,  3.9462e-01,  4.2854e-01,  2.5645e-02, -3.0715e-01,\n",
       "         3.7307e-03,  4.9375e-02,  4.4365e-02,  1.2281e-02,  2.7521e-01,\n",
       "        -4.3195e-02,  7.3216e-01,  5.4357e-01,  4.5817e-01,  5.2422e-01,\n",
       "         3.9883e-01,  9.6501e-03,  1.3321e-01,  5.0197e-01,  6.5890e-01,\n",
       "         1.7619e-01, -2.7955e-02, -4.5098e-02,  2.0371e-02,  1.3561e-01,\n",
       "         2.4991e-01,  5.2013e-01,  3.5961e-01,  3.5625e-01,  2.0558e-01,\n",
       "        -5.7780e-02, -3.7089e-01, -5.0716e-01,  2.6936e-01, -8.5981e-02,\n",
       "         7.1465e-01,  4.8228e-01, -5.5658e-01,  4.7892e-03,  4.5647e-01,\n",
       "        -5.8211e-01,  1.4198e-01,  2.4764e-01,  3.5498e-01,  3.7642e-01,\n",
       "         7.4813e-02,  5.3848e-01,  7.1022e-01, -7.5658e-04,  2.1168e-01,\n",
       "         8.3868e-02, -1.1069e-01, -6.5368e-01, -2.9257e-01, -3.2406e-01,\n",
       "         3.8233e-01,  4.9510e-01,  7.8964e-02, -7.5198e-01,  2.9037e-01,\n",
       "         3.5464e-01, -1.6829e-01,  4.2121e-01, -3.4791e-01, -5.8247e-01,\n",
       "         1.5644e-01,  1.9125e-01,  3.2261e-01,  3.5351e-02,  4.3879e-01,\n",
       "         9.3819e-01, -7.7736e-02,  1.4310e-01,  1.8860e-01,  2.8068e-01,\n",
       "         5.0132e-01,  3.2322e-01,  2.2143e-01,  6.2201e-01,  7.7561e-01,\n",
       "         2.7110e-01,  2.5221e-02,  2.3296e-01, -2.3808e-01,  6.8711e-01,\n",
       "         2.2095e-01,  8.0995e-03,  1.8516e-01,  2.1676e-01, -2.9988e-01,\n",
       "         5.3839e-01,  2.5203e-01,  7.7074e-01,  1.2431e-01,  6.4092e-01,\n",
       "         5.6938e-01,  6.5329e-01,  7.3129e-01, -1.1808e-01,  6.0116e-01,\n",
       "         3.5424e-01,  1.9180e-01,  8.0052e-01, -2.2328e-02,  4.0051e-01,\n",
       "         3.9902e-01,  9.4780e-02,  1.7127e-01,  9.2256e-01,  3.7678e-01,\n",
       "         3.6584e-01,  1.7220e-01,  4.6287e-01,  3.4191e-01,  2.9895e-01,\n",
       "        -1.3121e-01,  4.2902e-01,  7.5438e-02,  7.2728e-01,  3.3034e-01,\n",
       "         1.7193e-01,  2.9672e-01,  4.8840e-01,  9.2671e-01,  2.3367e-01,\n",
       "         4.8308e-04,  1.8924e-01,  5.9782e-01,  4.4096e-01,  1.0306e+00,\n",
       "         1.0849e-01, -3.6887e-01,  3.1257e-02,  1.1182e-01,  5.0567e-01,\n",
       "         2.6857e-01,  9.7140e-01,  1.8527e-01, -2.7811e-01, -1.7135e-01,\n",
       "        -6.6358e-02, -2.9738e-01, -2.0802e-01,  1.7280e-01,  6.5379e-01,\n",
       "        -2.7789e-01, -4.6215e-02,  1.1851e-01,  9.7384e-02,  6.1801e-01,\n",
       "         2.2165e-01,  3.9540e-01,  6.0712e-01, -2.8928e-01,  5.0572e-02,\n",
       "         3.8229e-01, -1.0567e-02,  1.1807e+00,  2.2969e-01, -1.9130e-02,\n",
       "         2.7082e-02,  6.4796e-01,  1.9247e-01,  2.9719e-01,  3.4385e-02,\n",
       "         2.8349e-01, -6.7003e-01, -1.6572e-01,  1.1240e-01,  4.3386e-01,\n",
       "         5.0303e-01,  1.2584e-02,  1.7475e-02, -5.3541e-01,  6.1457e-01,\n",
       "         5.5227e-01,  4.4089e-01,  5.3646e-01,  2.1049e-01,  7.1359e-01,\n",
       "         3.2364e-01,  2.8720e-01,  4.7554e-01,  6.4550e-01,  9.3528e-02,\n",
       "         4.2005e-01,  4.6181e-02,  1.4968e-01,  2.7950e-01, -5.5331e-01,\n",
       "         1.1145e-01,  5.6586e-01,  4.8897e-03,  3.6113e-01,  2.5833e-03,\n",
       "         4.0422e-01, -5.7216e-01,  2.6435e-01,  1.1443e-01, -6.7810e-01,\n",
       "        -8.3365e-01, -3.9288e-01, -1.7591e-01, -9.2112e-02, -1.4068e-01,\n",
       "         9.9213e-03,  3.9269e-01,  3.3533e-02, -2.1754e-01,  1.4324e-01,\n",
       "        -7.7510e-02,  2.1787e-01,  4.0500e-02,  5.7276e-01,  1.8193e-02,\n",
       "         4.8839e-01, -4.4512e-01,  1.0773e-01,  3.3458e-01,  2.8053e-01,\n",
       "         2.2442e-01, -1.9210e-01,  1.8582e-01, -4.5534e-01, -3.8761e-01,\n",
       "         9.3213e-03, -5.7089e-01, -2.2193e-01,  4.9027e-01, -3.6663e-02,\n",
       "        -7.6633e-01,  5.3901e-02,  3.3462e-01,  2.7685e-02,  3.9002e-01,\n",
       "         6.5254e-03,  9.6249e-02, -2.9332e-01,  4.3110e-01, -1.2202e-01,\n",
       "        -3.6369e-01,  3.3761e-02,  3.9864e-01,  3.5036e-01,  8.8115e-01,\n",
       "        -1.5243e-02,  4.8471e-01,  3.3062e-01, -1.0487e-01,  4.4765e-02,\n",
       "         8.2926e-01, -6.7028e-02,  2.3732e-01,  5.8373e-01, -5.1296e-01,\n",
       "        -7.6040e-01,  4.0180e-01,  6.2632e-01,  2.4497e-01, -1.6881e-01,\n",
       "         6.3092e-01,  8.5019e-01,  1.3439e-01,  2.7591e-01, -1.6767e-01,\n",
       "        -9.5433e-02, -1.6635e-01, -4.6173e-01,  3.0120e-02,  1.0161e+00,\n",
       "         2.9235e-01,  4.1557e-01, -5.8754e-02,  5.5468e-01,  6.6206e-01,\n",
       "         6.5786e-01,  1.7131e-01,  8.6967e-01,  1.9826e-01,  1.2657e-01,\n",
       "         1.1833e-01,  1.2442e-01,  7.1357e-01,  5.6912e-01,  3.0606e-01,\n",
       "         8.5263e-01,  6.8039e-01, -2.4331e-01,  5.7300e-01,  9.5774e-01,\n",
       "         3.7025e-01,  2.8239e-01,  4.7626e-01,  2.1981e-01,  1.0952e-01,\n",
       "         4.2365e-01,  1.8664e-01, -1.0646e-01, -1.4823e-01,  3.3610e-02,\n",
       "        -2.4232e-02,  2.8867e-01,  8.9875e-02,  3.3132e-01,  2.8077e-01,\n",
       "         3.9109e-01,  2.7434e-01,  1.6445e-01,  3.7070e-01,  1.9538e-01,\n",
       "         4.5809e-02,  2.4668e-01,  8.5487e-02, -2.6590e-03,  1.0002e-01,\n",
       "         3.7226e-02,  2.6144e-01, -9.2450e-03,  1.2415e-01,  9.8272e-04,\n",
       "        -2.0861e-01, -1.3299e-01, -1.2535e-01,  1.0287e-01,  2.3137e-02,\n",
       "        -2.2234e-02,  1.0174e-02], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.start_logits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNuIY6ukFSnf"
   },
   "source": [
    "以下结果说明，第 4 个问答中，第 306 个 token 最有可能是 start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBsLnYBTEMDp",
    "outputId": "a1d78c43-0ad0-4303-c635-2feae4f6c58f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([342, 290,  23, 306, 490, 308, 282,  12, 312, 315, 250, 300, 243, 277,\n",
       "        509,   8], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs.start_logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "XOlVfrfJGKJA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
    "end_pred = torch.argmax(outputs.end_logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCgnLNW6GLVz",
    "outputId": "85cfac89-4919-4ab5-b22c-c59c716ac6bd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([342, 290,  23, 306, 490, 308, 282,  12, 312, 315, 250, 300, 243, 277,\n",
       "        509,   8], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugKtENtzGNfY",
    "outputId": "3836dd89-fc0c-4583-ccc6-7d18ccf17dcb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 52,   1, 483,   1,  39,   0,  49,  95, 138,  78, 227, 391,  70, 189,\n",
       "        124, 112], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLRkqpVP--g0",
    "outputId": "04d3f55a-4f51-4768-9e80-9a18b4b80a10",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 634/634 [04:39<00:00,  2.27it/s, Epoch 0, train loss: 2.0812, acc start: 0.4286, acc end: 0.4266]\n",
      "Epoch 1: 100%|██████████| 634/634 [04:39<00:00,  2.27it/s, Epoch 1, train loss: 1.1397, acc start: 0.6018, acc end: 0.6130]\n",
      "Epoch 2: 100%|██████████| 634/634 [04:40<00:00,  2.26it/s, Epoch 2, train loss: 0.7619, acc start: 0.7039, acc end: 0.7072]\n"
     ]
    }
   ],
   "source": [
    "if fp16_training:\n",
    "    model, optim, train_loader = accelerator.prepare(model, optim, train_loader)\n",
    "    \n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    loss_sum = 0.0\n",
    "    acc_start_sum = 0.0\n",
    "    acc_end_sum = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        start_positions = batch['start_positions']\n",
    "        end_positions = batch['end_positions']\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, \n",
    "                        start_positions=start_positions, \n",
    "                        end_positions=end_positions)\n",
    "        \n",
    "        loss = outputs.loss.mean()\n",
    "        if fp16_training:\n",
    "            accelerator.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # Obtain answer by choosing the most probable start position / end position\n",
    "        # Using `torch.argmax` and its `dim` parameter to extract preditions for start position and end position.\n",
    "        start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
    "        end_pred = torch.argmax(outputs.end_logits, dim=1)\n",
    "        \n",
    "        # calculate accuracy for start and end positions. eg., using start_pred and start_positions to calculate acc_start.\n",
    "        acc_start = (start_pred == start_positions).float().mean()\n",
    "        acc_end = (end_pred == end_positions).float().mean()\n",
    "        ### END CODE HERE ### \n",
    "        \n",
    "        acc_start_sum += acc_start\n",
    "        acc_end_sum += acc_end\n",
    "        \n",
    "        # Update progress bar\n",
    "        postfix = {\n",
    "            \"loss\": f\"{loss_sum/(batch_idx+1):.4f}\",\n",
    "            \"acc_start\": f\"{acc_start_sum/(batch_idx+1):.4f}\",\n",
    "            \"acc_end\": f\"{acc_end_sum/(batch_idx+1):.4f}\"\n",
    "        }\n",
    "\n",
    "        # Add batch accuracy to progress bar\n",
    "        batch_desc = f\"Epoch {epoch}, train loss: {postfix['loss']}\"\n",
    "        pbar.set_postfix_str(f\"{batch_desc}, acc start: {postfix['acc_start']}, acc end: {postfix['acc_end']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "unZT1GHn4yOx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(doc, query):\n",
    "    print(doc)\n",
    "    print('提问：', query)\n",
    "    item = tokenizer([doc, query], max_length=512, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        input_ids = item['input_ids'].to(device).reshape(1,-1)\n",
    "        attention_mask = item['attention_mask'].to(device).reshape(1,-1)\n",
    "        \n",
    "        outputs = model(input_ids[:, :512], attention_mask[:, :512])\n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # Using `torch.argmax` and its `dim` parameter to extract preditions for start position and end position.\n",
    "        start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
    "        end_pred = torch.argmax(outputs.end_logits, dim=1)\n",
    "        ### END CODE HERE ### \n",
    "    \n",
    "    try:\n",
    "        start_pred = item.token_to_chars(0, start_pred)\n",
    "        end_pred = item.token_to_chars(0, end_pred)\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    if start_pred.start > end_pred.end:\n",
    "        return ''\n",
    "    else:\n",
    "        return doc[start_pred.start:end_pred.end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "YUIztgBv4yOx",
    "outputId": "8cc460e9-80cf-4b74-81e6-04dbb2cff8cb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraphs': [{'id': 'DEV_109',\n",
       "   'context': '岑朗天（），笔名朗天、霍惊觉。香港作家、影评人、文化活动策划、大学兼职讲师。香港新亚研究所硕士，师从牟宗三，父亲为香港专栏作家昆南。曾在香港多家报社从事繙译、编辑、采访工作。1995年加入香港电影评论学会，并于2003-2007年出任该会会长，2016年退出。1995年参与创立新研哲学会，后易名香港人文哲学会，再易名香港人文学会。1998年加入树宁．现在式单位，出任该剧团董事及编剧。2003年担任牛棚书展（2003-6）统筹，协助开拓主流以外的书展文化（牛棚书展精神后为九龙城书节继承）。2004年6月至2011年加入商业电台光明顶，担任嘉宾主持。2004年至2014年于香港中文大学新闻与传播学院兼职教授媒体创意写作。2012年始兼任香港浸会大学电影学院讲师，教授文学与影视相关课程。',\n",
       "   'qas': [{'question': '岑朗天笔名叫什么？',\n",
       "     'id': 'DEV_109_QUERY_0',\n",
       "     'answers': [{'text': '朗天、霍惊觉', 'answer_start': 8},\n",
       "      {'text': '朗天、霍惊觉', 'answer_start': 8},\n",
       "      {'text': '朗天、霍惊觉', 'answer_start': 8}]},\n",
       "    {'question': '岑朗天的职业都有哪些？',\n",
       "     'id': 'DEV_109_QUERY_1',\n",
       "     'answers': [{'text': '作家、影评人、文化活动策划、大学兼职讲师', 'answer_start': 17},\n",
       "      {'text': '作家、影评人、文化活动策划、大学兼职讲师', 'answer_start': 17},\n",
       "      {'text': '作家、影评人、文化活动策划、大学兼职讲师', 'answer_start': 17}]},\n",
       "    {'question': '岑朗天哪年加入香港电影评论学会？',\n",
       "     'id': 'DEV_109_QUERY_2',\n",
       "     'answers': [{'text': '1995年', 'answer_start': 87},\n",
       "      {'text': '1995年', 'answer_start': 87},\n",
       "      {'text': '1995年', 'answer_start': 87}]},\n",
       "    {'question': '2004年6月至2011年，岑朗天在什么地方担任嘉宾主持？',\n",
       "     'id': 'DEV_109_QUERY_3',\n",
       "     'answers': [{'text': '商业电台光明顶', 'answer_start': 261},\n",
       "      {'text': '商业电台光明顶', 'answer_start': 261},\n",
       "      {'text': '商业电台光明顶', 'answer_start': 261}]},\n",
       "    {'question': '2004年至2014年，岑朗天在香港中文大学哪个学院教授课程？',\n",
       "     'id': 'DEV_109_QUERY_4',\n",
       "     'answers': [{'text': '新闻与传播学院', 'answer_start': 294},\n",
       "      {'text': '新闻与传播学院', 'answer_start': 294},\n",
       "      {'text': '新闻与传播学院', 'answer_start': 294}]}]}],\n",
       " 'id': 'DEV_109',\n",
       " 'title': '岑朗天'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev['data'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "vAgxwNam4yOx",
    "outputId": "ab98d157-3b6c-4ac4-fc31-071052452bfd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "岑朗天（），笔名朗天、霍惊觉。香港作家、影评人、文化活动策划、大学兼职讲师。香港新亚研究所硕士，师从牟宗三，父亲为香港专栏作家昆南。曾在香港多家报社从事繙译、编辑、采访工作。1995年加入香港电影评论学会，并于2003-2007年出任该会会长，2016年退出。1995年参与创立新研哲学会，后易名香港人文哲学会，再易名香港人文学会。1998年加入树宁．现在式单位，出任该剧团董事及编剧。2003年担任牛棚书展（2003-6）统筹，协助开拓主流以外的书展文化（牛棚书展精神后为九龙城书节继承）。2004年6月至2011年加入商业电台光明顶，担任嘉宾主持。2004年至2014年于香港中文大学新闻与传播学院兼职教授媒体创意写作。2012年始兼任香港浸会大学电影学院讲师，教授文学与影视相关课程。\n",
      "提问： 岑朗天笔名叫什么？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'朗天、霍惊觉'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "predict(dev['data'][100]['paragraphs'][0]['context'],\n",
    "       dev['data'][100]['paragraphs'][0]['qas'][0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc='温开宇在美国波士顿大学读数理金融和金融科技硕士，他的身高是193厘米。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "温开宇在美国波士顿大学读数理金融和金融科技硕士，他的身高是193厘米。\n",
      "提问： 温开宇在哪个大学？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'美国波士顿大学'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\n",
    "    doc=doc,\n",
    "    query='温开宇在哪个大学？'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "温开宇在美国波士顿大学读数理金融和金融科技硕士，他的身高是193厘米。\n",
      "提问： 温开宇读什么专业？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'数理金融和金融科技硕士'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\n",
    "    doc=doc,\n",
    "    query='温开宇读什么专业？'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "温开宇在美国波士顿大学读数理金融和金融科技硕士，他的身高是193厘米。\n",
      "提问： 温开宇多高？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'193厘米'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\n",
    "    doc=doc,\n",
    "    query='温开宇多高？'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "温开宇在美国波士顿大学读数理金融和金融科技硕士，他的身高是193厘米。\n",
      "提问： 这段话的主角叫什么名字？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'温开宇'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\n",
    "    doc=doc,\n",
    "    query='这段话的主角叫什么名字？'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对于期货资产，投资者既可以做多也可以做空，因此\"上涨趋势\"和\"下跌趋势\"都可以作为交易信号。我们将平均最大回撤和平均最大反向回撤中较小的那一个，作为市场情绪平稳度指标。市场情绪平稳度指标越小，则上涨或者下跌的趋势越强。然后我们再根据具体是上涨还是下跌的趋势，即可判断交易方向。\n",
      "提问： 可以做多也可以做空的资产是什么？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'期货资产'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\n",
    "    doc='对于期货资产，投资者既可以做多也可以做空，因此\"上涨趋势\"和\"下跌趋势\"都可以作为交易信号。我们将平均最大回撤和平均最大反向回撤中较小的那一个，作为市场情绪平稳度指标。市场情绪平稳度指标越小，则上涨或者下跌的趋势越强。然后我们再根据具体是上涨还是下跌的趋势，即可判断交易方向。',\n",
    "    query='可以做多也可以做空的资产是什么？'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 对于单 GPU 训练的模型，直接用 .save_pretrained()\n",
    "# model.save_pretrained(\"./fengchao-bert-qa\", from_pt=True) \n",
    "# 对于多 GPU 训练得到的模型，要加上 .module\n",
    "model.module.save_pretrained(\"./fengchao-bert-qa\", from_pt=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T15:06:49.735459Z",
     "iopub.status.busy": "2023-04-16T15:06:49.734283Z",
     "iopub.status.idle": "2023-04-16T15:06:51.118662Z",
     "shell.execute_reply": "2023-04-16T15:06:51.117617Z",
     "shell.execute_reply.started": "2023-04-16T15:06:49.735399Z"
    }
   },
   "outputs": [],
   "source": [
    "model_load_from_file = BertForQuestionAnswering.from_pretrained('./fengchao-bert-qa').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T15:08:08.695411Z",
     "iopub.status.busy": "2023-04-16T15:08:08.694390Z",
     "iopub.status.idle": "2023-04-16T15:08:08.703808Z",
     "shell.execute_reply": "2023-04-16T15:08:08.702641Z",
     "shell.execute_reply.started": "2023-04-16T15:08:08.695369Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_local(doc, query):\n",
    "    print(doc)\n",
    "    print('提问：', query)\n",
    "    item = tokenizer([doc, query], max_length=512, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        input_ids = item['input_ids'].to(device).reshape(1,-1)\n",
    "        attention_mask = item['attention_mask'].to(device).reshape(1,-1)\n",
    "        \n",
    "        outputs = model_load_from_file(input_ids[:, :512], attention_mask[:, :512])\n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        # Using `torch.argmax` and its `dim` parameter to extract preditions for start position and end position.\n",
    "        start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
    "        end_pred = torch.argmax(outputs.end_logits, dim=1)\n",
    "        ### END CODE HERE ### \n",
    "    \n",
    "    try:\n",
    "        start_pred = item.token_to_chars(0, start_pred)\n",
    "        end_pred = item.token_to_chars(0, end_pred)\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    if start_pred.start > end_pred.end:\n",
    "        return ''\n",
    "    else:\n",
    "        return doc[start_pred.start:end_pred.end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T15:08:28.188905Z",
     "iopub.status.busy": "2023-04-16T15:08:28.188518Z",
     "iopub.status.idle": "2023-04-16T15:08:28.210266Z",
     "shell.execute_reply": "2023-04-16T15:08:28.209157Z",
     "shell.execute_reply.started": "2023-04-16T15:08:28.188870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "温开宇在美国波士顿大学读数理金融和金融科技硕士，他的身高是193厘米。\n",
      "提问： 温开宇多高？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'193厘米'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_local(\n",
    "    doc=doc,\n",
    "    query='温开宇多高？'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T15:12:35.369259Z",
     "iopub.status.busy": "2023-04-16T15:12:35.368586Z",
     "iopub.status.idle": "2023-04-16T15:12:36.426278Z",
     "shell.execute_reply": "2023-04-16T15:12:36.425041Z",
     "shell.execute_reply.started": "2023-04-16T15:12:35.369216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "388M\tfengchao-bert-qa\n"
     ]
    }
   ],
   "source": [
    "!du -h fengchao-bert-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZzWaC8q4yOx"
   },
   "source": [
    "## Open Questions\n",
    "可以查阅相关资料，并完成如下开放式的问答题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5Q4n_UA4yOx"
   },
   "source": [
    "- 我们使用了512长度的Bert，但是在实际应用中，输入长度可能大于512，你想怎么解决这个问题，请描述你的算法，在训练和预测时分别采取什么样的方法。（假设问题的长度都满足小于512token，段落的长度可能大于512token，以QA问题为例）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAbMYlQG4yOy"
   },
   "source": [
    "Your Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hGXhdfr4yOy"
   },
   "source": [
    "- 在输出中，我们分别对start_pred和end_pred的位置进行预估，如果end_pred<start_pred，我们可以如何解决这样的问题?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94R7Dtc84yOy"
   },
   "source": [
    "Your Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hd2Wq9j4yOy"
   },
   "source": [
    "- Bert的分词方式是什么?在中文中，你觉得这样的方式会带来什么问题？什么样的分词方式适合中文？在中文的文本上，除了改变分词方式，还有哪些方式可以提升模型效果？\n",
    "\n",
    "阅读资料：https://github.com/ymcui/Chinese-BERT-wwm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EazfnlFe4yOy"
   },
   "source": [
    "Your Answer:"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
