{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeremy-feng/deep-learning-coursework/blob/main/2-BERT/BERT-QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XF4wAqb4yOr"
      },
      "source": [
        "# 作业2：对Bert进行微调，完成QA任务"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeoLEi2r4yOt"
      },
      "source": [
        "**如果你对Bert没有了解，请先观看视频 [BERT 论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1PL411M7eQ)**\n",
        "\n",
        "注：本次作业并不需要预先了解任何Transformer的知识，如有兴趣，可以在观看Bert的视频前，先预习 [Transformer论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1pu411o7BE)，后续课程中会讲解Transformer的知识。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUrLd3HX71mx",
        "outputId": "e2420273-e24c-40d4-9ceb-fef9785999bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WG84QaHe4yOu",
        "outputId": "9d4d1073-7a14-4c68-c8a0-bd5c1aec0443",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [],
        "id": "261wXIir4yOv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "same_seeds(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xDKJnLW24yOv"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/deep-learning/2-BERT/cmrc2018/train.json') as f:\n",
        "    train = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/deep-learning/2-BERT/cmrc2018/dev.json') as f:\n",
        "    dev = json.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKFgHzeK4yOv"
      },
      "source": [
        "Let's have a glance at the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [],
        "id": "k8-oDM8-4yOv",
        "outputId": "db5fc040-d3f0-49dc-9c88-3f671bb7a3cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
        "\n",
        "# You can explore more pretrained models from https://huggingface.co/models\n",
        "# 这段代码利用Hugging Face库中的BertTokenizerFast方法从预训练模型'bert-base-chinese'中加载tokenizer。\n",
        "# 这个预训练模型是一个中文BERT模型，可以将中文句子或文本数据转换为相应的token，以便进行文本分类、序列标注等自然语言处理任务。\n",
        "# BertTokenizerFast是BertTokenizer的升级版，速度更快，性能更优。\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-chinese').to(device)\n",
        "\n",
        "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm-mWAfp4yOv"
      },
      "source": [
        "## PreProcessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWLFri4V4yOv"
      },
      "source": [
        "### Prepare training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "W4Vq3rCC4yOv"
      },
      "outputs": [],
      "source": [
        "paragraphs = []\n",
        "questions = []\n",
        "start_positions = []\n",
        "end_positions = []\n",
        "for paragraph in train['data']:\n",
        "    for qa in paragraph['paragraphs'][0]['qas']:\n",
        "        \n",
        "        ### START CODE HERE ### \n",
        "        # For each question, add its paragraph, question, start_position and end_position(after calculation) to its corresponding list.\n",
        "        paragraphs.append(paragraph['paragraphs'][0]['context'])\n",
        "        questions.append(qa['question'])\n",
        "        start_position = qa['answers'][0]['answer_start']\n",
        "        start_positions.append(start_position)\n",
        "        anwser_length = len(qa['answers'][0]['text'])\n",
        "        end_positions.append(start_position + anwser_length)\n",
        "        ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYpWVxhvE_5l",
        "outputId": "55fa7c9d-1594-440d-c7e4-56581cb365d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['范廷颂是什么时候被任为主教的？',\n",
              " '1990年，范廷颂担任什么职务？',\n",
              " '范廷颂是于何时何地出生的？',\n",
              " '1994年3月，范廷颂担任什么职务？',\n",
              " '范廷颂是何时去世的？']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_positions[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxwRkayQFGXR",
        "outputId": "91fe224b-7871-4e6c-ef47-555cf471535a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[30, 41, 97, 548, 759]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end_positions[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pshpsS3BFJFh",
        "outputId": "3e1b98f9-bba9-4ceb-844b-2926d150828a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[35, 62, 126, 598, 780]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "查看第 3 个问题的回答是否正确"
      ],
      "metadata": {
        "id": "LbQdrGUFilrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs[0][start_positions[2]: end_positions[2]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hXwZrYtQiqWY",
        "outputId": "623bcfb6-869c-43df-f815-3d941b741cb2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "将 paragraphs 和 questions 进行 encoding。\n",
        "参考：[https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__)"
      ],
      "metadata": {
        "id": "aAf1IrcPGGch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "tags": [],
        "id": "BtlaETbX4yOw"
      },
      "outputs": [],
      "source": [
        "# 下面这段代码使用了 Hugging Face 的 tokenizer 方法，将 paragraphs 和 questions 转换成相应的 token，\n",
        "# 返回一个字典 (train_encodings) 包含这些 token 的各种信息，这些信息包括 input_ids、attention_mask 等等。\n",
        "# return_tensors='pt' 表示返回 PyTorch 下的 tensor 格式\n",
        "# padding 用于填充不足 max_length 的 token\n",
        "# truncation 用于在超过 max_length 时截断 token\n",
        "# 最终的 token 长度被限制在 512 内\n",
        "train_encodings = tokenizer(\n",
        "    paragraphs,\n",
        "    questions,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQAzqnsvIFqD",
        "outputId": "1fa893cf-ed13-478f-b2f9-5e2ffe9b0475"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX4vyeGS4yOw"
      },
      "source": [
        "- 在问答任务中，`input_ids` 是将输入文本转换为整数序列后的输出。它将每个单词或子词映射到一个唯一的整数 ID, 位于 [CLS] 和 [SEP] 标记会被分别映射到一个特殊的 ID，(101: CLS, 102: SEP)。具体可以参考下方例子。\n",
        "\n",
        "- 在 `token_type_ids` 中，这些标记的值通常为 0 或 1，其中 0 表示该 token 属于第一个文本序列（通常是问题），1 表示该 token 属于第二个文本序列（通常是段落）。\n",
        "\n",
        "- 在 `attention_mask` 中，0 表示对应的标记应该被忽略，1 表示对应的标记应该被关注。当输入序列长度不足最大长度时，我们需要在序列末尾填充一些无意义的标记，以使序列长度达到最大长度。在这种情况下，`tokenizer`将填充的标记的 attention mask 设置为 0，以告诉模型它们不应该被关注。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = tokenizer(\n",
        "    paragraphs[0],\n",
        "    questions[0],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        ")['input_ids'][0]"
      ],
      "metadata": {
        "id": "ZeEfNKVyWAen"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = tokenizer(\n",
        "    paragraphs[0],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        ")['input_ids'][0]"
      ],
      "metadata": {
        "id": "CQbP9QEJZ-xS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "tags": [],
        "id": "-TCD9U374yOw",
        "outputId": "d6b6733b-df66-4150-e6c1-7648e6f013ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'为 天 主 教 河 内 总 教 区 宗 座 署 [SEP] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenizer.decode([711, 1921,  712, 3136, 3777, 1079, 2600, 3136, 1277,\n",
        "        2134, 2429, 5392,  102, 5745, 2455, 7563, 3221,  784,  720, 3198,  952,\n",
        "        6158,  818,  711,  712, 3136, 4638, 8043,  102])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len([5745, 2455, 7563, 3221,  784,  720, 3198,  952,\n",
        "        6158,  818,  711,  712, 3136, 4638, 8043,  102])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l80_9ZsedeMq",
        "outputId": "2570c87a-4ba7-428b-a10b-d290645de170"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "最后 16 个元素为 1，这说明最后 16 个 token 对应的是 question"
      ],
      "metadata": {
        "id": "2knZqNTO6ePb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings['token_type_ids'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ9lOj7vdqkS",
        "outputId": "7dc3b5c1-e3ec-4dd2-fc34-bfcecf168cca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings['token_type_ids'][0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHmjrpNqdgV3",
        "outputId": "b3483015-f0f5-40f1-977c-5fa58ab8d191"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(16)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([711, 1921,  712, 3136, 3777, 1079, 2600, 3136, 1277,\n",
        "        2134, 2429, 5392, 4415,  809, 1856, 6133, 6421, 3136, 1277, 2600,  712,\n",
        "        3136, 4638, 4958, 5375,  511, 8447, 2399,  102])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "seEeGEBLdDsp",
        "outputId": "ce72ea78-62e5-43d4-c73d-d7e961ed543e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings['token_type_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izDDbdndLW9m",
        "outputId": "02ee7315-cdee-4b11-8694-b48f44e1af18"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
              "        [0, 0, 0,  ..., 1, 1, 1],\n",
              "        [0, 0, 0,  ..., 1, 1, 1],\n",
              "        ...,\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "tags": [],
        "id": "dimamZ1Q4yOw",
        "outputId": "78491690-7b59-47a3-9c13-6b83f39847b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10142, 512])\n",
            "torch.Size([10142, 512])\n",
            "torch.Size([10142, 512])\n"
          ]
        }
      ],
      "source": [
        "print(train_encodings['input_ids'].shape)\n",
        "print(train_encodings['token_type_ids'].shape)\n",
        "print(train_encodings['attention_mask'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面的代码的作用是：将 answer 在原始 paragrapgh 的起止索引，转换为在经过tokenizor 之后点 input_ids 中的起止索引"
      ],
      "metadata": {
        "id": "nok-nu4jg6Av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `char_to_token` will convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
        "train_encodings['start_positions'] = torch.tensor([train_encodings.char_to_token(idx, x) if train_encodings.char_to_token(idx, x) != None else -1\n",
        "                                      for idx, x in enumerate(start_positions)])\n",
        "train_encodings['end_positions'] = torch.tensor([train_encodings.char_to_token(idx, x-1) if train_encodings.char_to_token(idx, x-1) != None else -1\n",
        "                                    for idx, x in enumerate(end_positions)])"
      ],
      "metadata": {
        "id": "xPfmrljfKmal"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings['start_positions']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ok3cM76hjDc",
        "outputId": "e2aae299-28ef-4f81-8462-a0e446af859d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 31,  39,  86,  ..., 142, 225,  17])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings['end_positions']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCjrEnwghkpZ",
        "outputId": "c27374e8-c216-40bd-d7b2-06dd55b66f03"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 32,  56, 110,  ..., 143, 244,  19])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "以第 3 个问题为例，即以 86 和 110 作为起止索引为例，查看它们对应的字符串是什么"
      ],
      "metadata": {
        "id": "covx-s_zh_Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2MVfIHvSiRgF",
        "outputId": "ad66e184-6550-4ed2-c33f-17f0e211aa5c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'范廷颂是于何时何地出生的？'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs[0][start_positions[2]: end_positions[2]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LcgO8Dk6hynX",
        "outputId": "597a27a9-1641-45f1-9cf4-1ed6c1e7bdef"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(train_encodings['input_ids'][0][86: 110+1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J5mqFCD3horn",
        "outputId": "d1082623-4bdf-4313-e7de-62df3b0270a6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBxjey-94yOw"
      },
      "source": [
        "### Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "tags": [],
        "id": "n6j6_ndI4yOw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "    \n",
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {k: v[idx].to(device) for k, v in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrtMKHnx4yOw"
      },
      "source": [
        "Automatic Mixed Precision (AMP) is available on NVIDIA GPUs that support Tensor Cores, which are specialized hardware units for performing fast matrix multiplication and convolution operations in deep learning. Specifically, Tensor Cores are available on NVIDIA Volta, Turing, and Ampere architectures, which include the following GPU series:\n",
        "\n",
        "- Volta: Tesla V100, Titan V\n",
        "- Turing: Quadro RTX, GeForce RTX 20-series, Titan RTX\n",
        "- Ampere: A100, GeForce RTX 30-series, Titan RTX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "tags": [],
        "id": "renVJ9D-4yOx",
        "outputId": "4b99918d-b02b-4a82-93cb-16cf3cedd169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.18.0\n"
          ]
        }
      ],
      "source": [
        "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\n",
        "fp16_training = True\n",
        "\n",
        "if fp16_training:\n",
        "    !pip install accelerate\n",
        "    from accelerate import Accelerator\n",
        "\n",
        "    accelerator = Accelerator()\n",
        "    device = accelerator.device\n",
        "\n",
        "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFviEE_F8oii",
        "outputId": "07889db5-1aaf-4f2c-c1a2-455f1d966114"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaFhtF97-glu",
        "outputId": "29e80e84-e934-40d2-d145-9f8894b0db1e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0262,  0.0109, -0.0187,  ...,  0.0903,  0.0028,  0.0064],\n",
              "        [ 0.0021,  0.0216,  0.0011,  ...,  0.0809,  0.0018,  0.0249],\n",
              "        [ 0.0147,  0.0005,  0.0028,  ...,  0.0836,  0.0121,  0.0282],\n",
              "        ...,\n",
              "        [ 0.0346,  0.0021,  0.0085,  ...,  0.0085,  0.0337,  0.0099],\n",
              "        [ 0.0541,  0.0289,  0.0263,  ...,  0.0526,  0.0651,  0.0353],\n",
              "        [ 0.0200,  0.0023, -0.0089,  ...,  0.0799, -0.0562,  0.0247]],\n",
              "       device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(model.parameters()).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdMC5I8d-Jyt",
        "outputId": "9cb111c5-7ee5-4b23-d9e4-5824ed2bd5d8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([21128, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "tags": [],
        "id": "rmerLUao4yOx"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "### START CODE HERE ### \n",
        "# Use AdamW as the optimizer, and learning rate 5e-5.\n",
        "# https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "### END CODE HERE ### \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, optim, train_loader = accelerator.prepare(model, optim, train_loader)"
      ],
      "metadata": {
        "id": "y5zUdeMdAjOz"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "loss_sum = 0.0\n",
        "acc_start_sum = 0.0\n",
        "acc_end_sum = 0.0"
      ],
      "metadata": {
        "id": "ehKlMKzzAjtn"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
        "    print(batch_idx, batch)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnjdfXlsAnyy",
        "outputId": "b32dbf64-6b10-4b0b-ce3e-c9634699de38"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1268 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 {'input_ids': tensor([[ 101, 7770, 7965,  ...,  720, 8043,  102],\n",
            "        [ 101,  730, 3780,  ...,    0,    0,    0],\n",
            "        [ 101,  517, 1520,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2476, 5739,  ...,    0,    0,    0],\n",
            "        [ 101,  517, 2769,  ...,    0,    0,    0],\n",
            "        [ 101, 1765, 1898,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'start_positions': tensor([ 14,  57, 136,  43, 120, 307,  53, 204], device='cuda:0'), 'end_positions': tensor([ 20,  59, 137,  52, 121, 311,  55, 213], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-guALDwAr03",
        "outputId": "4514bf48-defa-4e87-e793-8301927c23c5"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "由于 train_loader 设置了 batch_size=8，因此这里每一个批次包含 8 个数据\n",
        "\n"
      ],
      "metadata": {
        "id": "xSup9yI_BtYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGq5IQyfBFo3",
        "outputId": "4b263c75-399e-4f20-a394-582c38bf0feb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 7770, 7965,  ...,  720, 8043,  102],\n",
              "         [ 101,  730, 3780,  ...,    0,    0,    0],\n",
              "         [ 101,  517, 1520,  ...,    0,    0,    0],\n",
              "         ...,\n",
              "         [ 101, 2476, 5739,  ...,    0,    0,    0],\n",
              "         [ 101,  517, 2769,  ...,    0,    0,    0],\n",
              "         [ 101, 1765, 1898,  ...,    0,    0,    0]], device='cuda:0'),\n",
              " 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
              " 'start_positions': tensor([ 14,  57, 136,  43, 120, 307,  53, 204], device='cuda:0'),\n",
              " 'end_positions': tensor([ 20,  59, 137,  52, 121, 311,  55, 213], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim.zero_grad()"
      ],
      "metadata": {
        "id": "KNMkO5oYCU30"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = batch['input_ids']\n",
        "attention_mask = batch['attention_mask']\n",
        "start_positions = batch['start_positions']\n",
        "end_positions = batch['end_positions']\n",
        "\n",
        "outputs = model(input_ids, attention_mask=attention_mask, \n",
        "                start_positions=start_positions, \n",
        "                end_positions=end_positions)"
      ],
      "metadata": {
        "id": "GuPEKq_DCXs2"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH2jT4s7Ccy9",
        "outputId": "37109c11-231e-49ab-8469-3ed74c100d66"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionAnsweringModelOutput(loss=tensor(6.4114, device='cuda:0', grad_fn=<DivBackward0>), start_logits=tensor([[ 0.1751, -0.1216,  0.4636,  ...,  0.5935,  0.4839,  0.7081],\n",
              "        [-0.1473, -0.3509,  0.1302,  ..., -0.2002, -0.3946, -0.3744],\n",
              "        [ 0.2786,  0.0641,  0.4189,  ...,  0.1372,  0.2105,  0.1368],\n",
              "        ...,\n",
              "        [-0.0034,  0.1997,  0.1095,  ...,  0.2093,  0.1921,  0.0117],\n",
              "        [ 0.5537, -0.0444,  0.5240,  ...,  0.3153,  0.1386,  0.0803],\n",
              "        [ 0.0620,  0.0415,  0.1780,  ...,  0.0031, -0.2416, -0.1601]],\n",
              "       device='cuda:0', grad_fn=<CloneBackward0>), end_logits=tensor([[ 0.8021,  0.6094, -0.2039,  ...,  0.5187,  0.6640,  0.2209],\n",
              "        [ 0.1915,  0.7896, -0.2102,  ...,  0.0642,  0.2232,  0.0899],\n",
              "        [ 0.3394,  0.5749,  0.5838,  ..., -0.1420, -0.1652, -0.0355],\n",
              "        ...,\n",
              "        [ 0.4693,  0.5062, -0.0599,  ..., -0.0544, -0.2077, -0.1598],\n",
              "        [ 0.7608,  0.4795, -0.1417,  ..., -0.0799, -0.2762,  0.1039],\n",
              "        [ 0.8589,  0.9004, -0.2578,  ...,  0.0463,  0.1590,  0.3712]],\n",
              "       device='cuda:0', grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = outputs.loss"
      ],
      "metadata": {
        "id": "UY1BDazICsjQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVz5XXcMCt0n",
        "outputId": "91839d86-9bd2-4bf3-b07e-a623d7f413d4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.4114, device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accelerator.backward(loss)"
      ],
      "metadata": {
        "id": "K8zfgU2fCyn6"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim.step()"
      ],
      "metadata": {
        "id": "DFWK6kxKC008"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_sum += loss.item()"
      ],
      "metadata": {
        "id": "fqULWs1MC48Z"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "对于第一个问答，输出每个 token 作为 start 的 logit，其值越大，说明越有可能作为 start"
      ],
      "metadata": {
        "id": "VsGgLXaJFAE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.start_logits[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLtJO3k6Eic_",
        "outputId": "32d7a3d7-5de0-4184-aed7-11e3de433b14"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.7514e-01, -1.2164e-01,  4.6360e-01,  3.0065e-01,  2.9359e-01,\n",
              "         5.4023e-01,  3.7192e-01,  2.0874e-01,  1.7231e-01,  7.1841e-01,\n",
              "         1.3333e-01,  5.4579e-01,  5.5089e-01,  5.3872e-01,  4.7586e-01,\n",
              "         7.1148e-01,  6.5530e-01,  8.3357e-01,  6.0856e-01,  5.7281e-01,\n",
              "         1.1920e-01, -1.5845e-01,  6.6009e-02, -2.3724e-02, -1.2359e-01,\n",
              "        -2.5370e-01, -2.3190e-01,  3.7006e-01,  4.1520e-01,  2.9374e-01,\n",
              "         3.1413e-01, -8.0037e-02,  1.8278e-03,  7.2287e-01,  1.0099e-01,\n",
              "         2.3198e-01,  9.7436e-02,  5.8377e-01,  2.3440e-01,  8.6625e-01,\n",
              "         6.2343e-01,  3.7204e-01,  3.1254e-01,  1.0517e+00,  1.2295e-01,\n",
              "         9.6936e-02,  8.8488e-01,  2.4407e-01, -3.6297e-01, -3.4003e-02,\n",
              "         7.0640e-02,  2.9501e-01, -5.1674e-01,  6.4323e-01, -1.1721e-01,\n",
              "         5.3592e-01,  2.5208e-01,  2.3752e-01, -2.9283e-01,  4.7447e-02,\n",
              "        -3.3944e-02,  2.2520e-01, -1.1115e-01,  1.9890e-01,  3.2585e-01,\n",
              "         5.5852e-02,  5.1859e-01,  3.4758e-04, -8.0967e-02, -1.6597e-01,\n",
              "        -7.5633e-02,  1.0486e-01, -3.4242e-01, -4.8884e-01, -7.1005e-01,\n",
              "         7.6043e-02, -5.1927e-01, -3.5362e-01, -5.3071e-01, -1.6062e-01,\n",
              "         4.3857e-01, -9.8716e-02, -4.2249e-02,  3.7888e-02,  2.3918e-01,\n",
              "        -3.9354e-02, -6.9290e-01, -4.3374e-01, -2.2767e-01,  1.2874e-01,\n",
              "         4.3299e-01,  1.6119e-02, -4.7591e-01, -1.4184e-01,  3.9956e-01,\n",
              "         1.2387e-01,  3.0346e-01, -4.4740e-02, -2.0762e-01,  5.3449e-01,\n",
              "         1.6321e-01, -5.0829e-02, -4.3788e-02, -3.1347e-01,  2.3334e-01,\n",
              "         3.3515e-01, -6.9034e-01,  1.8224e-01,  9.7853e-02,  2.1267e-03,\n",
              "         4.0947e-01,  3.4893e-01,  1.3622e-01,  5.1217e-01, -3.3256e-01,\n",
              "        -2.0588e-01,  2.6687e-01,  5.9704e-01,  4.5490e-01, -7.7767e-02,\n",
              "         1.5351e-01,  8.5600e-02,  4.0374e-01,  5.8347e-02, -1.1434e-01,\n",
              "         1.5016e-01,  8.1319e-01,  4.9073e-01,  4.0982e-01,  2.1072e-01,\n",
              "         9.2564e-02, -2.8774e-01,  7.1728e-03,  2.5622e-01,  1.2377e-01,\n",
              "        -6.4453e-02,  7.4423e-02,  9.5984e-02, -9.4502e-02,  4.0109e-01,\n",
              "        -2.1145e-01,  2.3097e-02, -2.3402e-02,  1.9795e-01, -6.1313e-03,\n",
              "        -3.9595e-01, -2.4830e-01,  7.7448e-02, -1.0893e-01, -4.9436e-02,\n",
              "        -3.6945e-01, -5.1585e-01, -3.7484e-01,  5.2651e-02, -2.2681e-01,\n",
              "        -2.6020e-01,  2.0564e-03, -5.3994e-01, -5.9635e-01, -6.6150e-02,\n",
              "        -4.9710e-01, -3.4067e-01, -5.7928e-02, -2.1889e-01,  2.4642e-01,\n",
              "         1.3482e-01, -5.6330e-02,  2.3552e-01, -9.5227e-02,  8.8413e-03,\n",
              "         1.8711e-01,  3.4167e-01,  1.1712e-01, -1.3491e-01, -9.9140e-01,\n",
              "        -1.4019e-01, -3.7289e-01,  3.3701e-01, -1.2846e-01, -1.4016e-01,\n",
              "        -7.0727e-02, -2.9497e-03,  7.7019e-02, -4.1940e-02, -2.5191e-01,\n",
              "        -9.3498e-01,  5.0158e-01, -8.8216e-02, -6.9006e-02, -2.6083e-01,\n",
              "        -1.1475e-01,  1.0596e-01,  2.3176e-01,  9.4022e-01, -1.9474e-01,\n",
              "        -1.8381e-01,  1.4759e-01,  3.2779e-01, -2.8225e-01,  4.4270e-01,\n",
              "         1.2893e-01, -2.4543e-01,  8.0682e-02,  1.9397e-01,  1.7793e-01,\n",
              "        -4.2575e-01, -1.9992e-01, -2.7710e-01,  5.2218e-01, -1.9261e-01,\n",
              "         1.1165e-01, -3.0399e-01,  1.9147e-01, -9.3450e-02,  2.8783e-01,\n",
              "         3.6041e-01,  9.6803e-02, -2.5849e-01, -4.9265e-01,  1.5741e-01,\n",
              "         3.1194e-01,  1.0087e-01,  3.0766e-01,  1.0540e-01,  4.4282e-01,\n",
              "        -1.5104e-01,  1.7029e-01, -5.5370e-01, -2.3228e-01, -3.8013e-01,\n",
              "        -4.6612e-01,  1.4178e-01,  3.6227e-01, -1.5567e-01,  2.7909e-01,\n",
              "        -1.7596e-01, -3.1047e-01, -4.7921e-01, -4.1570e-01, -2.3140e-01,\n",
              "         1.2212e-01, -2.8514e-01,  6.4783e-02,  4.4239e-01, -2.4220e-01,\n",
              "        -3.8667e-01, -3.4669e-01, -3.9081e-01,  3.6286e-01,  1.8179e-01,\n",
              "         1.1132e-01,  5.6491e-02, -1.2173e-01,  4.6282e-01,  1.4621e-01,\n",
              "         5.8941e-01, -8.0843e-01,  4.6079e-01,  8.2111e-01,  1.8782e-01,\n",
              "        -1.3086e-01,  1.6225e-01, -1.4371e-01, -1.7830e-01, -2.0077e-01,\n",
              "         4.0427e-01,  2.4860e-02,  6.3590e-01,  4.5255e-01, -4.7967e-02,\n",
              "         1.5312e-01,  6.4032e-01, -5.4073e-02,  4.5630e-01,  1.9654e-01,\n",
              "         1.0042e-01,  1.1720e-01,  2.8623e-01, -2.9093e-02,  2.2794e-01,\n",
              "         3.3824e-01,  4.6348e-01,  4.3250e-01,  1.6190e-02, -1.5139e-01,\n",
              "         6.2736e-01, -1.7460e-01,  3.2632e-01,  3.8124e-02,  3.2944e-01,\n",
              "         2.7635e-01,  2.2512e-01,  8.3332e-01,  1.1803e+00,  2.3191e-01,\n",
              "         3.5692e-01, -4.6956e-01,  8.0073e-01,  6.8730e-01,  6.2085e-02,\n",
              "         1.8687e-01,  5.5110e-01,  6.1140e-02,  2.3051e-01, -3.8860e-01,\n",
              "        -4.1641e-01,  1.2965e+00,  4.2165e-01,  5.0603e-01,  4.7113e-01,\n",
              "         5.5976e-01,  1.2343e-01,  2.7924e-01, -4.6819e-01,  2.2992e-01,\n",
              "         3.5128e-01,  1.0484e-01,  2.2612e-01,  5.2448e-01,  5.3644e-01,\n",
              "         5.3223e-01, -1.0746e-01, -1.4722e-01, -5.0841e-01,  3.8730e-01,\n",
              "         3.8931e-01, -6.7667e-02,  3.7953e-01,  7.3014e-01,  5.1750e-01,\n",
              "         4.2395e-01,  9.6288e-01,  2.9318e-01, -3.8823e-02,  6.2156e-02,\n",
              "         3.4598e-01,  8.8763e-02,  1.5711e-01,  2.3478e-01,  1.0208e+00,\n",
              "         8.3892e-01, -4.6983e-02,  5.2156e-01,  5.1773e-01,  1.8202e-01,\n",
              "         2.9968e-01,  9.1927e-02,  4.8835e-01,  1.2659e-02, -6.9168e-01,\n",
              "         2.3394e-01,  3.7049e-01,  1.8996e-01,  1.5234e-01,  3.3106e-01,\n",
              "        -2.3631e-01, -1.9608e-01,  1.0399e-01, -4.8062e-01,  3.0299e-01,\n",
              "        -2.0669e-01,  5.6168e-01, -2.4375e-01, -2.0000e-01,  3.3829e-01,\n",
              "         3.4561e-01, -9.5348e-02, -3.8557e-01,  4.4722e-01,  6.0359e-01,\n",
              "        -3.1278e-01,  2.0549e-01,  5.6225e-02,  3.4680e-02,  7.3900e-01,\n",
              "         2.6514e-01,  6.5646e-01, -4.5617e-01, -4.0604e-01, -3.3659e-01,\n",
              "         4.3728e-01,  3.8498e-01,  3.9495e-01, -1.1899e-01,  4.2692e-01,\n",
              "        -1.9768e-01, -2.8743e-01, -2.1985e-01,  2.5512e-01, -3.7942e-02,\n",
              "        -2.4126e-01,  2.0682e-01, -3.4009e-02, -3.5328e-01,  4.0816e-02,\n",
              "        -2.5055e-01, -7.9812e-02, -2.2431e-01, -4.2082e-01, -3.9751e-01,\n",
              "        -3.7038e-01, -6.8434e-01, -3.3184e-01, -4.0975e-01, -5.2955e-01,\n",
              "        -1.0593e-02, -3.6942e-01, -1.9901e-01, -5.1661e-01, -2.5726e-01,\n",
              "        -2.0151e-01, -1.5247e-01, -3.7673e-01, -5.9369e-01, -2.6317e-01,\n",
              "        -1.2443e-01, -2.9140e-01, -5.7580e-01, -3.4404e-01, -2.6687e-01,\n",
              "        -6.5301e-01,  4.4150e-02, -6.7212e-01,  3.8089e-02, -4.1594e-01,\n",
              "        -6.6521e-01, -7.8350e-01, -1.8564e-01,  1.0169e-01, -3.7695e-01,\n",
              "         1.6550e-01, -6.9468e-01,  2.4632e-02, -2.5475e-01, -4.9433e-01,\n",
              "        -7.9518e-02,  4.4056e-01,  1.7213e-01, -3.5872e-01, -1.1816e-02,\n",
              "         3.7186e-01,  1.1331e-01, -1.6625e-01,  9.9063e-01,  5.8747e-01,\n",
              "        -8.8488e-02,  1.7541e-01,  5.5709e-01,  4.7227e-02,  7.5906e-01,\n",
              "         2.0890e-01,  1.6157e-01, -2.7000e-01,  1.0948e-01, -2.3373e-01,\n",
              "         3.6449e-01,  1.8615e-02, -7.1577e-02, -9.9733e-02,  1.9547e-01,\n",
              "         7.1217e-01,  5.2430e-01, -5.4969e-02, -2.6389e-01,  8.3643e-01,\n",
              "         7.2896e-01,  3.0386e-01,  2.4535e-01,  5.7400e-01,  2.0999e-03,\n",
              "         4.9632e-01,  8.2642e-02,  2.4524e-01,  5.0674e-01,  4.8455e-02,\n",
              "         5.4774e-01, -3.0823e-01,  2.3912e-01, -4.2191e-01,  1.3536e-01,\n",
              "         6.7216e-01,  2.7245e-02,  4.1818e-01, -3.5315e-01,  2.5037e-01,\n",
              "         1.7945e-01,  4.4256e-01,  3.0332e-02,  5.1377e-01,  2.9235e-01,\n",
              "         9.3094e-03,  3.8857e-01,  2.0895e-01,  4.4767e-01,  2.2280e-01,\n",
              "         5.2007e-01,  1.4495e-01,  4.4869e-01,  4.4692e-01, -1.4197e-01,\n",
              "         6.8596e-01,  2.3752e-01,  8.3013e-01,  2.9843e-01,  2.1450e-01,\n",
              "         1.1968e-01,  7.6372e-01,  3.6150e-01,  6.8194e-01,  5.9347e-01,\n",
              "         4.8394e-01,  7.0808e-01], device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下结果说明，第 1 个问答中，第 306 个 token 最有可能是 start"
      ],
      "metadata": {
        "id": "lNuIY6ukFSnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(outputs.start_logits, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBsLnYBTEMDp",
        "outputId": "a1d78c43-0ad0-4303-c635-2feae4f6c58f"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([306,  35, 298,  20, 283,  62,  22, 285], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
        "end_pred = torch.argmax(outputs.end_logits, dim=1)"
      ],
      "metadata": {
        "id": "XOlVfrfJGKJA"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCgnLNW6GLVz",
        "outputId": "85cfac89-4919-4ab5-b22c-c59c716ac6bd"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([306,  35, 298,  20, 283,  62,  22, 285], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugKtENtzGNfY",
        "outputId": "3836dd89-fc0c-4583-ccc6-7d18ccf17dcb"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([143,   1, 234, 313, 198, 359, 196,   1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if fp16_training:\n",
        "    model, optim, train_loader = accelerator.prepare(model, optim, train_loader)\n",
        "    \n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    loss_sum = 0.0\n",
        "    acc_start_sum = 0.0\n",
        "    acc_end_sum = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        optim.zero_grad()\n",
        "        \n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        start_positions = batch['start_positions']\n",
        "        end_positions = batch['end_positions']\n",
        "        \n",
        "        outputs = model(input_ids, attention_mask=attention_mask, \n",
        "                        start_positions=start_positions, \n",
        "                        end_positions=end_positions)\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        if fp16_training:\n",
        "            accelerator.backward(loss)\n",
        "        else:\n",
        "            loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        loss_sum += loss.item()\n",
        "        \n",
        "        ### START CODE HERE ### \n",
        "        # Obtain answer by choosing the most probable start position / end position\n",
        "        # Using `torch.argmax` and its `dim` parameter to extract preditions for start position and end position.\n",
        "        start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
        "        end_pred = torch.argmax(outputs.end_logits, dim=1)\n",
        "        \n",
        "        # calculate accuracy for start and end positions. eg., using start_pred and start_positions to calculate acc_start.\n",
        "        acc_start = (start_pred == start_positions).float().mean()\n",
        "        acc_end = (end_pred == end_positions).float().mean()\n",
        "        ### END CODE HERE ### \n",
        "        \n",
        "        acc_start_sum += acc_start\n",
        "        acc_end_sum += acc_end\n",
        "        \n",
        "        # Update progress bar\n",
        "        postfix = {\n",
        "            \"loss\": f\"{loss_sum/(batch_idx+1):.4f}\",\n",
        "            \"acc_start\": f\"{acc_start_sum/(batch_idx+1):.4f}\",\n",
        "            \"acc_end\": f\"{acc_end_sum/(batch_idx+1):.4f}\"\n",
        "        }\n",
        "\n",
        "        # Add batch accuracy to progress bar\n",
        "        batch_desc = f\"Epoch {epoch}, train loss: {postfix['loss']}\"\n",
        "        pbar.set_postfix_str(f\"{batch_desc}, acc start: {postfix['acc_start']}, acc end: {postfix['acc_end']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLRkqpVP--g0",
        "outputId": "04d3f55a-4f51-4768-9e80-9a18b4b80a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:  50%|█████     | 639/1268 [08:59<08:53,  1.18it/s, Epoch 0, train loss: 2.4209, acc start: 0.3885, acc end: 0.3783]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "unZT1GHn4yOx"
      },
      "outputs": [],
      "source": [
        "def predcit(doc, query):\n",
        "    print(doc)\n",
        "    print('提问：', query)\n",
        "    item = tokenizer([doc, query], max_length=512, return_tensors='pt', truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        input_ids = item['input_ids'].to(device).reshape(1,-1)\n",
        "        attention_mask = item['attention_mask'].to(device).reshape(1,-1)\n",
        "        \n",
        "        outputs = model(input_ids[:, :512], attention_mask[:, :512])\n",
        "        \n",
        "        ### START CODE HERE ### \n",
        "        # Using `torch.argmax` and its `dim` parameter to extract preditions for start position and end position.\n",
        "        start_pred = \n",
        "        end_pred = \n",
        "        ### END CODE HERE ### \n",
        "    \n",
        "    try:\n",
        "        start_pred = item.token_to_chars(0, start_pred)\n",
        "        end_pred = item.token_to_chars(0, end_pred)\n",
        "    except:\n",
        "        return ''\n",
        "    \n",
        "    if start_pred.start > end_pred.end:\n",
        "        return ''\n",
        "    else:\n",
        "        return doc[start_pred.start:end_pred.end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YUIztgBv4yOx",
        "outputId": "8cc460e9-80cf-4b74-81e6-04dbb2cff8cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'paragraphs': [{'id': 'DEV_109',\n",
              "   'context': '岑朗天（），笔名朗天、霍惊觉。香港作家、影评人、文化活动策划、大学兼职讲师。香港新亚研究所硕士，师从牟宗三，父亲为香港专栏作家昆南。曾在香港多家报社从事繙译、编辑、采访工作。1995年加入香港电影评论学会，并于2003-2007年出任该会会长，2016年退出。1995年参与创立新研哲学会，后易名香港人文哲学会，再易名香港人文学会。1998年加入树宁．现在式单位，出任该剧团董事及编剧。2003年担任牛棚书展（2003-6）统筹，协助开拓主流以外的书展文化（牛棚书展精神后为九龙城书节继承）。2004年6月至2011年加入商业电台光明顶，担任嘉宾主持。2004年至2014年于香港中文大学新闻与传播学院兼职教授媒体创意写作。2012年始兼任香港浸会大学电影学院讲师，教授文学与影视相关课程。',\n",
              "   'qas': [{'question': '岑朗天笔名叫什么？',\n",
              "     'id': 'DEV_109_QUERY_0',\n",
              "     'answers': [{'text': '朗天、霍惊觉', 'answer_start': 8},\n",
              "      {'text': '朗天、霍惊觉', 'answer_start': 8},\n",
              "      {'text': '朗天、霍惊觉', 'answer_start': 8}]},\n",
              "    {'question': '岑朗天的职业都有哪些？',\n",
              "     'id': 'DEV_109_QUERY_1',\n",
              "     'answers': [{'text': '作家、影评人、文化活动策划、大学兼职讲师', 'answer_start': 17},\n",
              "      {'text': '作家、影评人、文化活动策划、大学兼职讲师', 'answer_start': 17},\n",
              "      {'text': '作家、影评人、文化活动策划、大学兼职讲师', 'answer_start': 17}]},\n",
              "    {'question': '岑朗天哪年加入香港电影评论学会？',\n",
              "     'id': 'DEV_109_QUERY_2',\n",
              "     'answers': [{'text': '1995年', 'answer_start': 87},\n",
              "      {'text': '1995年', 'answer_start': 87},\n",
              "      {'text': '1995年', 'answer_start': 87}]},\n",
              "    {'question': '2004年6月至2011年，岑朗天在什么地方担任嘉宾主持？',\n",
              "     'id': 'DEV_109_QUERY_3',\n",
              "     'answers': [{'text': '商业电台光明顶', 'answer_start': 261},\n",
              "      {'text': '商业电台光明顶', 'answer_start': 261},\n",
              "      {'text': '商业电台光明顶', 'answer_start': 261}]},\n",
              "    {'question': '2004年至2014年，岑朗天在香港中文大学哪个学院教授课程？',\n",
              "     'id': 'DEV_109_QUERY_4',\n",
              "     'answers': [{'text': '新闻与传播学院', 'answer_start': 294},\n",
              "      {'text': '新闻与传播学院', 'answer_start': 294},\n",
              "      {'text': '新闻与传播学院', 'answer_start': 294}]}]}],\n",
              " 'id': 'DEV_109',\n",
              " 'title': '岑朗天'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev['data'][100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "vAgxwNam4yOx",
        "outputId": "ab98d157-3b6c-4ac4-fc31-071052452bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "岑朗天（），笔名朗天、霍惊觉。香港作家、影评人、文化活动策划、大学兼职讲师。香港新亚研究所硕士，师从牟宗三，父亲为香港专栏作家昆南。曾在香港多家报社从事繙译、编辑、采访工作。1995年加入香港电影评论学会，并于2003-2007年出任该会会长，2016年退出。1995年参与创立新研哲学会，后易名香港人文哲学会，再易名香港人文学会。1998年加入树宁．现在式单位，出任该剧团董事及编剧。2003年担任牛棚书展（2003-6）统筹，协助开拓主流以外的书展文化（牛棚书展精神后为九龙城书节继承）。2004年6月至2011年加入商业电台光明顶，担任嘉宾主持。2004年至2014年于香港中文大学新闻与传播学院兼职教授媒体创意写作。2012年始兼任香港浸会大学电影学院讲师，教授文学与影视相关课程。\n",
            "提问： 岑朗天笔名叫什么？\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'朗天、霍惊觉'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "predcit(dev['data'][100]['paragraphs'][0]['context'],\n",
        "       dev['data'][100]['paragraphs'][0]['qas'][0]['question'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZzWaC8q4yOx"
      },
      "source": [
        "## Open Questions\n",
        "可以查阅相关资料，并完成如下开放式的问答题。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5Q4n_UA4yOx"
      },
      "source": [
        "- 我们使用了512长度的Bert，但是在实际应用中，输入长度可能大于512，你想怎么解决这个问题，请描述你的算法，在训练和预测时分别采取什么样的方法。（假设问题的长度都满足小于512token，段落的长度可能大于512token，以QA问题为例）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAbMYlQG4yOy"
      },
      "source": [
        "Your Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hGXhdfr4yOy"
      },
      "source": [
        "- 在输出中，我们分别对start_pred和end_pred的位置进行预估，如果end_pred<start_pred，我们可以如何解决这样的问题?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94R7Dtc84yOy"
      },
      "source": [
        "Your Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hd2Wq9j4yOy"
      },
      "source": [
        "- Bert的分词方式是什么?在中文中，你觉得这样的方式会带来什么问题？什么样的分词方式适合中文？在中文的文本上，除了改变分词方式，还有哪些方式可以提升模型效果？\n",
        "\n",
        "阅读资料：https://github.com/ymcui/Chinese-BERT-wwm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EazfnlFe4yOy"
      },
      "source": [
        "Your Answer:"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}